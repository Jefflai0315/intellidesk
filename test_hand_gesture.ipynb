{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[34875]: Class CaptureDelegate is implemented in both /Users/jefflai/Intellidesk/Posture-analysis-system-using-MediaPipe-Pose/myenv/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x108c28860) and /Users/jefflai/Intellidesk/Posture-analysis-system-using-MediaPipe-Pose/myenv/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x2820925e0). One of the two will be used. Which one is undefined.\n",
      "objc[34875]: Class CVWindow is implemented in both /Users/jefflai/Intellidesk/Posture-analysis-system-using-MediaPipe-Pose/myenv/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x106b48a68) and /Users/jefflai/Intellidesk/Posture-analysis-system-using-MediaPipe-Pose/myenv/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x282092630). One of the two will be used. Which one is undefined.\n",
      "objc[34875]: Class CVView is implemented in both /Users/jefflai/Intellidesk/Posture-analysis-system-using-MediaPipe-Pose/myenv/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x106b48a90) and /Users/jefflai/Intellidesk/Posture-analysis-system-using-MediaPipe-Pose/myenv/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x282092658). One of the two will be used. Which one is undefined.\n",
      "objc[34875]: Class CVSlider is implemented in both /Users/jefflai/Intellidesk/Posture-analysis-system-using-MediaPipe-Pose/myenv/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x106b48ab8) and /Users/jefflai/Intellidesk/Posture-analysis-system-using-MediaPipe-Pose/myenv/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x282092680). One of the two will be used. Which one is undefined.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "2023-12-21 13:24:46.479 Python[34875:7610996] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ok_gesture(hand_landmarks, image_width, image_height):\n",
    "    thumb_tip = hand_landmarks.landmark[4]\n",
    "    index_tip = hand_landmarks.landmark[8]\n",
    "\n",
    "    # Calculate the Euclidean distance between thumb tip and index fingertip\n",
    "    distance = ((thumb_tip.x - index_tip.x) ** 2 + (thumb_tip.y - index_tip.y) ** 2) ** 0.5\n",
    "\n",
    "    # Normalize the distance by the image dimensions\n",
    "    normalized_distance = distance / (image_width ** 2 + image_height ** 2) ** 0.5\n",
    "\n",
    "    # Check if the distance is below a certain threshold to consider them as touching\n",
    "    if normalized_distance < 0.05:  # Threshold, you might need to adjust this based on testing\n",
    "        # Additional checks for other fingers can be added here\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def height_gesture(hand_landmarks,image_width, image_height):\n",
    "    # Get the landmarks for the fingertips, MCP joints, and wrist\n",
    "    wrist = hand_landmarks.landmark[0]\n",
    "    thumb_tip = hand_landmarks.landmark[4]\n",
    "    thumb_mcp = hand_landmarks.landmark[2]\n",
    "    index_tip = hand_landmarks.landmark[8]\n",
    "    index_mcp = hand_landmarks.landmark[5]\n",
    "    middle_tip = hand_landmarks.landmark[12]\n",
    "    middle_mcp = hand_landmarks.landmark[9]\n",
    "    ring_tip = hand_landmarks.landmark[16]\n",
    "    ring_mcp = hand_landmarks.landmark[13]\n",
    "    pinky_tip = hand_landmarks.landmark[20]\n",
    "    pinky_mcp = hand_landmarks.landmark[17]\n",
    "\n",
    "    distance = ((thumb_tip.x - index_tip.x) ** 2 + (thumb_tip.y - index_tip.y) ** 2) ** 0.5\n",
    "\n",
    "    normalized_distance = distance / (image_width ** 2 + image_height ** 2) ** 0.5\n",
    "\n",
    "    # Check if the distance is below a certain threshold to consider them as touching\n",
    "    if normalized_distance < 0.05:  # Threshold, you might need to adjust this based on testing\n",
    "        # Additional checks for other fingers can be added here\n",
    "        if (thumb_tip.y < thumb_mcp.y and\n",
    "        index_tip.y < index_mcp.y and\n",
    "        middle_tip.y < middle_mcp.y and\n",
    "        ring_tip.y < ring_mcp.y and\n",
    "        pinky_tip.y < pinky_mcp.y):\n",
    "            return \"UP\"\n",
    "        if (thumb_tip.y > thumb_mcp.y and\n",
    "        index_tip.y > index_mcp.y and\n",
    "        middle_tip.y > middle_mcp.y and\n",
    "        ring_tip.y > ring_mcp.y and\n",
    "        pinky_tip.y > pinky_mcp.y):\n",
    "            return \"DOWN\"\n",
    "        \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def table_height(hand_landmarks, image_width, image_height):\n",
    "    # Get the landmarks for the fingertips, MCP joints, and wrist\n",
    "    index_tip_y = hand_landmarks.landmark[8].y * image_height\n",
    "    index_mcp_y = hand_landmarks.landmark[5].y * image_height\n",
    "    middle_tip_y = hand_landmarks.landmark[12].y * image_height\n",
    "    middle_mcp_y = hand_landmarks.landmark[9].y * image_height\n",
    "    ring_tip_y = hand_landmarks.landmark[16].y * image_height\n",
    "    ring_mcp_y = hand_landmarks.landmark[13].y * image_height\n",
    "    pinky_tip_y = hand_landmarks.landmark[20].y * image_height\n",
    "    pinky_mcp_y = hand_landmarks.landmark[17].y * image_height\n",
    "    thumb_tip_x = hand_landmarks.landmark[4].x * image_width\n",
    "    thumb_mcp_x = hand_landmarks.landmark[2].x * image_width\n",
    "\n",
    "    # Check for a fist gesture\n",
    "    if (index_tip_y > index_mcp_y and\n",
    "        middle_tip_y > middle_mcp_y and\n",
    "        ring_tip_y > ring_mcp_y and\n",
    "        pinky_tip_y > pinky_mcp_y ): \n",
    "        return \"Fist\"\n",
    "    # Check for gesture 1\n",
    "    elif index_tip_y < index_mcp_y and middle_tip_y > middle_mcp_y and ring_tip_y > ring_mcp_y and pinky_tip_y > pinky_mcp_y:\n",
    "        return \"Gesture 1\"\n",
    "    # Check for gesture 2\n",
    "    elif index_tip_y < index_mcp_y and middle_tip_y < middle_mcp_y and ring_tip_y > ring_mcp_y and pinky_tip_y > pinky_mcp_y:\n",
    "        return \"Gesture 2\"\n",
    "    # Check for gesture 3\n",
    "    elif index_tip_y < index_mcp_y and middle_tip_y < middle_mcp_y and ring_tip_y < ring_mcp_y and pinky_tip_y > pinky_mcp_y:\n",
    "        return \"Gesture 3\"\n",
    "    else:\n",
    "        return \"Unknown Gesture\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal sent for Gesture 1: Hold a fist to confirm\n",
      "Signal sent for Gesture 2: Hold a fist to confirm\n",
      "Signal sent for Gesture 1: Hold a fist to confirm\n",
      "Change table posture to Gesture 1\n",
      "Signal sent for Gesture 2: Hold a fist to confirm\n",
      "Change table posture to Gesture 2\n",
      "Signal sent for Gesture 3: Hold a fist to confirm\n",
      "Change table posture to Gesture 3\n",
      "Signal sent for Gesture 1: Hold a fist to confirm\n",
      "Signal sent for Gesture 2: Hold a fist to confirm\n",
      "Signal sent for Gesture 3: Hold a fist to confirm\n",
      "Change table posture to Gesture 3\n",
      "Signal sent for Gesture 1: Hold a fist to confirm\n",
      "Signal sent for Gesture 2: Hold a fist to confirm\n",
      "Signal sent for Gesture 3: Hold a fist to confirm\n",
      "Signal sent for Gesture 2: Hold a fist to confirm\n",
      "Change table posture to Gesture 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 65\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39m# Add your other processing here\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[39m# ...\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m     \u001b[39m# Show the processed image - convert it back to BGR to display with OpenCV\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39mHand Tracking\u001b[39m\u001b[39m'\u001b[39m, cv2\u001b[39m.\u001b[39mcvtColor(image, cv2\u001b[39m.\u001b[39mCOLOR_RGB2BGR))\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mif\u001b[39;00m cv2\u001b[39m.\u001b[39;49mwaitKey(\u001b[39m5\u001b[39;49m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m \u001b[39m==\u001b[39m \u001b[39m27\u001b[39m:\n\u001b[1;32m     66\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     68\u001b[0m cap\u001b[39m.\u001b[39mrelease()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# Replace this with the actual initialization of your video capture and hand tracking\n",
    "cap = cv2.VideoCapture(0)  # or whatever your source is\n",
    "# hands = SomeHandTrackingModule()  # initialize your hand tracking module here\n",
    "\n",
    "gesture_start_times = {'Gesture 1': None, 'Gesture 2': None, 'Gesture 3': None}\n",
    "fist_signal_sent = False\n",
    "current_gesture = None\n",
    "gesture_threshold = 3  # seconds\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    # Flip the image horizontally for a later selfie-view display\n",
    "    # Convert the BGR image to RGB\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image and detect hands\n",
    "    results = hands.process(image)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Assuming table_height function returns 'Gesture 1', 'Gesture 2', 'Gesture 3', or 'Fist'\n",
    "            detected_gesture = table_height(hand_landmarks, image_height, image_width)\n",
    "\n",
    "            # Check if the detected gesture is one we're tracking\n",
    "            if detected_gesture in gesture_start_times:\n",
    "                if current_gesture != detected_gesture:\n",
    "                    # New gesture detected, start timing\n",
    "                    current_gesture = detected_gesture\n",
    "                    gesture_start_times[detected_gesture] = time.time()\n",
    "                    fist_signal_sent = False  # Reset the fist signal\n",
    "                else:\n",
    "                    # Gesture is sustained, check duration\n",
    "                    if time.time() - gesture_start_times[detected_gesture] >= gesture_threshold:\n",
    "                        # Send a signal to hold a fist if it wasn't sent already\n",
    "                        if not fist_signal_sent:\n",
    "                            print(f\"Signal sent for {detected_gesture}: Hold a fist to confirm\")\n",
    "                            fist_signal_sent = True\n",
    "            elif detected_gesture == 'Fist' and fist_signal_sent:\n",
    "                # Fist detected after signal, confirm the posture change\n",
    "                print(f\"Change table posture to {current_gesture}\")\n",
    "                # Reset the gesture and fist signal\n",
    "                current_gesture = None\n",
    "                fist_signal_sent = False\n",
    "                for gesture in gesture_start_times:\n",
    "                    gesture_start_times[gesture] = None\n",
    "            else:\n",
    "                # Different gesture or no gesture detected, reset the current gesture and timer for the previous gesture\n",
    "                current_gesture = None\n",
    "                fist_signal_sent = False\n",
    "                for gesture in gesture_start_times:\n",
    "                    gesture_start_times[gesture] = None\n",
    "\n",
    "    # Add your other processing here\n",
    "    # ...\n",
    "\n",
    "    # Show the processed image - convert it back to BGR to display with OpenCV\n",
    "    cv2.imshow('Hand Tracking', cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "416662e0c1598f44d1c47321e3a641e8fddce6b1f381058eead826809d9b358e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
