{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[2047]: Class CaptureDelegate is implemented in both /Users/jefflai/Intellidesk/Posture-analysis-system-using-MediaPipe-Pose/myenv/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x15c9725e0) and /Users/jefflai/Intellidesk/Posture-analysis-system-using-MediaPipe-Pose/myenv/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x17abb4860). One of the two will be used. Which one is undefined.\n",
      "objc[2047]: Class CVWindow is implemented in both /Users/jefflai/Intellidesk/Posture-analysis-system-using-MediaPipe-Pose/myenv/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x15c972630) and /Users/jefflai/Intellidesk/Posture-analysis-system-using-MediaPipe-Pose/myenv/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x16d6f0a68). One of the two will be used. Which one is undefined.\n",
      "objc[2047]: Class CVView is implemented in both /Users/jefflai/Intellidesk/Posture-analysis-system-using-MediaPipe-Pose/myenv/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x15c972658) and /Users/jefflai/Intellidesk/Posture-analysis-system-using-MediaPipe-Pose/myenv/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x16d6f0a90). One of the two will be used. Which one is undefined.\n",
      "objc[2047]: Class CVSlider is implemented in both /Users/jefflai/Intellidesk/Posture-analysis-system-using-MediaPipe-Pose/myenv/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x15c972680) and /Users/jefflai/Intellidesk/Posture-analysis-system-using-MediaPipe-Pose/myenv/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x16d6f0ab8). One of the two will be used. Which one is undefined.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ok_gesture(hand_landmarks, image_width, image_height):\n",
    "    thumb_tip = hand_landmarks.landmark[4]\n",
    "    index_tip = hand_landmarks.landmark[8]\n",
    "\n",
    "    # Calculate the Euclidean distance between thumb tip and index fingertip\n",
    "    distance = ((thumb_tip.x - index_tip.x) ** 2 + (thumb_tip.y - index_tip.y) ** 2) ** 0.5\n",
    "\n",
    "    # Normalize the distance by the image dimensions\n",
    "    normalized_distance = distance / (image_width ** 2 + image_height ** 2) ** 0.5\n",
    "\n",
    "    # Check if the distance is below a certain threshold to consider them as touching\n",
    "    if normalized_distance < 0.05:  # Threshold, you might need to adjust this based on testing\n",
    "        # Additional checks for other fingers can be added here\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def height_gesture(hand_landmarks,image_width, image_height):\n",
    "    # Get the landmarks for the fingertips, MCP joints, and wrist\n",
    "    wrist = hand_landmarks.landmark[0]\n",
    "    thumb_tip = hand_landmarks.landmark[4]\n",
    "    thumb_mcp = hand_landmarks.landmark[2]\n",
    "    index_tip = hand_landmarks.landmark[8]\n",
    "    index_mcp = hand_landmarks.landmark[5]\n",
    "    middle_tip = hand_landmarks.landmark[12]\n",
    "    middle_mcp = hand_landmarks.landmark[9]\n",
    "    ring_tip = hand_landmarks.landmark[16]\n",
    "    ring_mcp = hand_landmarks.landmark[13]\n",
    "    pinky_tip = hand_landmarks.landmark[20]\n",
    "    pinky_mcp = hand_landmarks.landmark[17]\n",
    "\n",
    "    distance = ((thumb_tip.x - index_tip.x) ** 2 + (thumb_tip.y - index_tip.y) ** 2) ** 0.5\n",
    "\n",
    "    normalized_distance = distance / (image_width ** 2 + image_height ** 2) ** 0.5\n",
    "\n",
    "    # Check if the distance is below a certain threshold to consider them as touching\n",
    "    if normalized_distance < 0.05:  # Threshold, you might need to adjust this based on testing\n",
    "        # Additional checks for other fingers can be added here\n",
    "        if (thumb_tip.y < thumb_mcp.y and\n",
    "        index_tip.y < index_mcp.y and\n",
    "        middle_tip.y < middle_mcp.y and\n",
    "        ring_tip.y < ring_mcp.y and\n",
    "        pinky_tip.y < pinky_mcp.y):\n",
    "            return \"UP\"\n",
    "        if (thumb_tip.y > thumb_mcp.y and\n",
    "        index_tip.y > index_mcp.y and\n",
    "        middle_tip.y > middle_mcp.y and\n",
    "        ring_tip.y > ring_mcp.y and\n",
    "        pinky_tip.y > pinky_mcp.y):\n",
    "            return \"DOWN\"\n",
    "        \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def table_height(hand_landmarks, image_width, image_height):\n",
    "    # Get the landmarks for the fingertips, MCP joints, and wrist\n",
    "    index_tip_y = hand_landmarks.landmark[8].y * image_height\n",
    "    index_mcp_y = hand_landmarks.landmark[5].y * image_height\n",
    "    middle_tip_y = hand_landmarks.landmark[12].y * image_height\n",
    "    middle_mcp_y = hand_landmarks.landmark[9].y * image_height\n",
    "    ring_tip_y = hand_landmarks.landmark[16].y * image_height\n",
    "    ring_mcp_y = hand_landmarks.landmark[13].y * image_height\n",
    "    pinky_tip_y = hand_landmarks.landmark[20].y * image_height\n",
    "    pinky_mcp_y = hand_landmarks.landmark[17].y * image_height\n",
    "    thumb_tip_x = hand_landmarks.landmark[4].x * image_width\n",
    "    thumb_mcp_x = hand_landmarks.landmark[2].x * image_width\n",
    "\n",
    "    # Check for a fist gesture\n",
    "    if (index_tip_y > index_mcp_y and\n",
    "        middle_tip_y > middle_mcp_y and\n",
    "        ring_tip_y > ring_mcp_y and\n",
    "        pinky_tip_y > pinky_mcp_y ): \n",
    "        return \"Fist\"\n",
    "    # Check for gesture 1\n",
    "    elif index_tip_y < index_mcp_y and middle_tip_y > middle_mcp_y and ring_tip_y > ring_mcp_y and pinky_tip_y > pinky_mcp_y:\n",
    "        return \"Gesture 1\"\n",
    "    # Check for gesture 2\n",
    "    elif index_tip_y < index_mcp_y and middle_tip_y < middle_mcp_y and ring_tip_y > ring_mcp_y and pinky_tip_y > pinky_mcp_y:\n",
    "        return \"Gesture 2\"\n",
    "    # Check for gesture 3\n",
    "    elif index_tip_y < index_mcp_y and middle_tip_y < middle_mcp_y and ring_tip_y < ring_mcp_y and pinky_tip_y > pinky_mcp_y:\n",
    "        return \"Gesture 3\"\n",
    "    else:\n",
    "        return \"Unknown Gesture\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 15:43:13.170 Python[2047:24739] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal sent for Gesture 2: Hold a fist to confirm\n",
      "Change table posture to Gesture 2\n",
      "Signal sent for Gesture 1: Hold a fist to confirm\n",
      "Change table posture to Gesture 1\n",
      "Signal sent for Gesture 3: Hold a fist to confirm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(cv2\u001b[39m.\u001b[39mflip(image, \u001b[39m1\u001b[39m), cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     24\u001b[0m \u001b[39m# Process the image and detect hands\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m results \u001b[39m=\u001b[39m hands\u001b[39m.\u001b[39;49mprocess(image)\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mmulti_hand_landmarks:\n\u001b[1;32m     27\u001b[0m     \u001b[39mfor\u001b[39;00m hand_landmarks \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mmulti_hand_landmarks:\n\u001b[1;32m     28\u001b[0m         \u001b[39m# Assuming table_height function returns 'Gesture 1', 'Gesture 2', 'Gesture 3', or 'Fist'\u001b[39;00m\n",
      "File \u001b[0;32m~/Intellidesk/Posture-analysis-system-using-MediaPipe-Pose/myenv/lib/python3.10/site-packages/mediapipe/python/solutions/hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NamedTuple:\n\u001b[1;32m    133\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39m         right hand) of the detected hand.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mprocess(input_data\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m: image})\n",
      "File \u001b[0;32m~/Intellidesk/Posture-analysis-system-using-MediaPipe-Pose/myenv/lib/python3.10/site-packages/mediapipe/python/solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    359\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    361\u001b[0m         stream\u001b[39m=\u001b[39mstream_name,\n\u001b[1;32m    362\u001b[0m         packet\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    363\u001b[0m                                  data)\u001b[39m.\u001b[39mat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 365\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49mwait_until_idle()\n\u001b[1;32m    366\u001b[0m \u001b[39m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m# output stream names.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m solution_outputs \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mnamedtuple(\n\u001b[1;32m    369\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSolutionOutputs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_stream_type_info\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# Replace this with the actual initialization of your video capture and hand tracking\n",
    "cap = cv2.VideoCapture(0)  # or whatever your source is\n",
    "# hands = SomeHandTrackingModule()  # initialize your hand tracking module here\n",
    "\n",
    "gesture_start_times = {'Gesture 1': None, 'Gesture 2': None, 'Gesture 3': None}\n",
    "fist_signal_sent = False\n",
    "current_gesture = None\n",
    "gesture_threshold = 3  # seconds\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    # Flip the image horizontally for a later selfie-view display\n",
    "    # Convert the BGR image to RGB\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image and detect hands\n",
    "    results = hands.process(image)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Assuming table_height function returns 'Gesture 1', 'Gesture 2', 'Gesture 3', or 'Fist'\n",
    "            detected_gesture = table_height(hand_landmarks, image_height, image_width)\n",
    "\n",
    "            # Check if the detected gesture is one we're tracking\n",
    "            if detected_gesture in gesture_start_times:\n",
    "                if current_gesture != detected_gesture:\n",
    "                    # New gesture detected, start timing\n",
    "                    current_gesture = detected_gesture\n",
    "                    gesture_start_times[detected_gesture] = time.time()\n",
    "                    fist_signal_sent = False  # Reset the fist signal\n",
    "                else:\n",
    "                    # Gesture is sustained, check duration\n",
    "                    if time.time() - gesture_start_times[detected_gesture] >= gesture_threshold:\n",
    "                        # Send a signal to hold a fist if it wasn't sent already\n",
    "                        if not fist_signal_sent:\n",
    "                            print(f\"Signal sent for {detected_gesture}: Hold a fist to confirm\")\n",
    "                            fist_signal_sent = True\n",
    "            elif detected_gesture == 'Fist' and fist_signal_sent:\n",
    "                # Fist detected after signal, confirm the posture change\n",
    "                print(f\"Change table posture to {current_gesture}\")\n",
    "                # Reset the gesture and fist signal\n",
    "                current_gesture = None\n",
    "                fist_signal_sent = False\n",
    "                for gesture in gesture_start_times:\n",
    "                    gesture_start_times[gesture] = None\n",
    "            else:\n",
    "                # Different gesture or no gesture detected, reset the current gesture and timer for the previous gesture\n",
    "                current_gesture = None\n",
    "                fist_signal_sent = False\n",
    "                for gesture in gesture_start_times:\n",
    "                    gesture_start_times[gesture] = None\n",
    "\n",
    "    # Add your other processing here\n",
    "    # ...\n",
    "\n",
    "    # Show the processed image - convert it back to BGR to display with OpenCV\n",
    "    cv2.imshow('Hand Tracking', cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "416662e0c1598f44d1c47321e3a641e8fddce6b1f381058eead826809d9b358e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
